{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "W3D3_Tutorial1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thor4/nma-dl-pytorch/blob/master/tutorials/W3D3_ReinforcementLearningForGames/student/W3D3_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "BX9WGFssgWgC"
      },
      "source": [
        "# Tutorial 1: Learn to play games with RL\n",
        "\n",
        "**Week 3, Day 3: Reinforcement Learning for Games**\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Mandana Samiei, Raymond Chua, Tim Lilicrap, Blake Richards\n",
        "\n",
        "__Content reviewers:__ Arush Tagade, Lily Cheng, Melvin Selim Atay, Kelson Shilling-Scrivo\n",
        "\n",
        "__Content editors:__ Melvin Selim Atay, Spiros Chavlis\n",
        "\n",
        "__Production editors:__ Namrata Bafna, Spiros Chavlis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "LubUPZtigWgF"
      },
      "source": [
        "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
        "\n",
        "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "uF66hheggWgG"
      },
      "source": [
        "---\n",
        "# Tutorial Objectives\n",
        "\n",
        "In this tutotial, you will learn how to implement a game loop and improve the performance of a random player. \n",
        "\n",
        "The specific objectives for this tutorial:\n",
        "*   Understand the format of two-players games\n",
        "*   Learn about value network and policy network\n",
        "\n",
        "In the Bonus sections you will learn about Monte Carlo Tree Search (MCTS) and compare its performance to policy-based and value-based players."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "yLsKMgsPgWgI"
      },
      "source": [
        "# @title Tutorial slides\n",
        "\n",
        "# @markdown These are the slides for the videos in the tutorial\n",
        "\n",
        "# @markdown If you want to locally download the slides, click [here](https://osf.io/3zn9w/download)\n",
        "from IPython.display import IFrame\n",
        "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/3zn9w/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "g57c743ZgWgJ"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "xqJN0yR0gWgK",
        "outputId": "74ecda66-83f9-42ad-a64a-bb1abe1bc977",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# @title Install dependencies\n",
        "!pip install coloredlogs --quiet\n",
        "\n",
        "!pip install git+https://github.com/NeuromatchAcademy/evaltools --quiet\n",
        "from evaltools.airtable import AirtableForm\n",
        "\n",
        "# generate airtable form\n",
        "atform = AirtableForm('appn7VdPRseSoMXEG','W3D3_T1','https://portal.neuromatchacademy.org/api/redirect/to/2baacd95-3fb5-4399-bf95-bbe5de255d2b')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███████▏                        | 10 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 20 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 30 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 40 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 46 kB 1.5 MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |███▉                            | 10 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 20 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 30 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 40 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 51 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 61 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 71 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 81 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 86 kB 3.8 MB/s \n",
            "\u001b[?25h  Building wheel for evaltools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "BSdmozrYgWgL"
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import logging\n",
        "import coloredlogs\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from pickle import Unpickler\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "coloredlogs.install(level='INFO')  # Change this to DEBUG to see more info."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "tmdQgha4gWgN"
      },
      "source": [
        "# @title Set random seed\n",
        "\n",
        "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
        "\n",
        "# for DL its critical to set the random seed so that students can have a\n",
        "# baseline to compare their results to expected results.\n",
        "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "eIhGddFAgWgO"
      },
      "source": [
        "# @title Set device (GPU or CPU). Execute `set_device()`\n",
        "# especially if torch modules used.\n",
        "\n",
        "# inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "f1r7ddzagWgP",
        "outputId": "f0b67b81-4661-42a7-ae1f-b53544c6d487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "SEED = 2021\n",
        "set_seed(seed=SEED)\n",
        "DEVICE = set_device()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random seed 2021 has been set.\n",
            "GPU is enabled in this notebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "n_jmEYY-gWgQ",
        "outputId": "3de02f7d-edd8-4250-e87a-54653f7f716a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# @title Download the modules\n",
        "\n",
        "# @markdown Run this cell!\n",
        "\n",
        "# @markdown Download from OSF. Original repo: https://github.com/raymondchua/nma_rl_games.git\n",
        "\n",
        "import os, io, sys, shutil, zipfile\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# download from github repo directly\n",
        "#!git clone git://github.com/raymondchua/nma_rl_games.git --quiet\n",
        "REPO_PATH = 'nma_rl_games'\n",
        "\n",
        "if os.path.exists(REPO_PATH):\n",
        "  download_string = \"Redownloading\"\n",
        "  shutil.rmtree(REPO_PATH)\n",
        "else:\n",
        "  download_string = \"Downloading\"\n",
        "\n",
        "zipurl = 'https://osf.io/kf4p9/download'\n",
        "print(f\"{download_string} and unzipping the file... Please wait.\")\n",
        "with urlopen(zipurl) as zipresp:\n",
        "  with zipfile.ZipFile(io.BytesIO(zipresp.read())) as zfile:\n",
        "    zfile.extractall()\n",
        "print(\"Download completed.\")\n",
        "\n",
        "print(f\"Add the {REPO_PATH} in the path and import the modules.\")\n",
        "# add the repo in the path\n",
        "sys.path.append('nma_rl_games/alpha-zero')\n",
        "\n",
        "# @markdown Import modules designed for use in this notebook\n",
        "import Arena\n",
        "\n",
        "from utils import *\n",
        "from Game import Game\n",
        "from MCTS import MCTS\n",
        "from NeuralNet import NeuralNet\n",
        "\n",
        "from othello.OthelloPlayers import *\n",
        "from othello.OthelloLogic import Board\n",
        "from othello.OthelloGame import OthelloGame\n",
        "from othello.pytorch.NNet import NNetWrapper as NNet"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading and unzipping the file... Please wait.\n",
            "Download completed.\n",
            "Add the nma_rl_games in the path and import the modules.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-aUKWueSgWgQ"
      },
      "source": [
        "The hyperparameters used throughout the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "HWeejg0AgWgR"
      },
      "source": [
        "args = dotdict({\n",
        "    'numIters': 1,            # in training setting this was 1000 and num of episodes=100\n",
        "    'numEps': 1,              # Number of complete self-play games to simulate during a new iteration.\n",
        "    'tempThreshold': 15,      # To control exploration and exploitation\n",
        "    'updateThreshold': 0.6,   # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
        "    'maxlenOfQueue': 200,     # Number of game examples to train the neural networks.\n",
        "    'numMCTSSims': 15,        # Number of games moves for MCTS to simulate.\n",
        "    'arenaCompare': 10,       # Number of games to play during arena play to determine if new net will be accepted.\n",
        "    'cpuct': 1,\n",
        "    'maxDepth':5,             # Maximum number of rollouts\n",
        "    'numMCsims': 5,           # Number of monte carlo simulations\n",
        "    'mc_topk': 3,             # top k actions for monte carlo rollout\n",
        "\n",
        "    'checkpoint': './temp/',\n",
        "    'load_model': False,\n",
        "    'load_folder_file': ('/dev/models/8x100x50','best.pth.tar'),\n",
        "    'numItersForTrainExamplesHistory': 20,\n",
        "\n",
        "    # define neural network arguments\n",
        "    'lr': 0.001,               # lr: learning rate\n",
        "    'dropout': 0.3,\n",
        "    'epochs': 10,\n",
        "    'batch_size': 64,\n",
        "    'device': DEVICE,\n",
        "    'num_channels': 512,\n",
        "})"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "W34NrxEAgWgS"
      },
      "source": [
        "---\n",
        "# Section 0: Introduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "zwYep2n-gWgS"
      },
      "source": [
        "# @title Video 0: Introduction\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1Yh411B7EP\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"5kQ-xGbjlJo\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Video 0: Introduction')\n",
        "\n",
        "display(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "OjCUJV1OgWgS"
      },
      "source": [
        "---\n",
        "# Section 1: Create a game/agent loop for RL\n",
        "\n",
        "*Time estimate: ~15mins*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "QWPZNEctgWgT"
      },
      "source": [
        "# @title Video 1: A game loop for RL\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1Wy4y1V7bt\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"aH2Hs8f6KrQ\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Video 1: A game loop for RL')\n",
        "\n",
        "display(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ICNWu2hngWgT"
      },
      "source": [
        "\n",
        "***Goal***: How to setup a game environment with multiple players for reinforcement learning experiments.\n",
        "\n",
        "***Exercise***: \n",
        "\n",
        "\n",
        "*   Build an agent that plays random moves\n",
        "*   Connect with connect 4 game\n",
        "*   Generate games including wins and losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "Nei87BvLgWgT"
      },
      "source": [
        "class OthelloGame(Game):\n",
        "  square_content = {\n",
        "      -1: \"X\",\n",
        "      +0: \"-\",\n",
        "      +1: \"O\"\n",
        "      }\n",
        "\n",
        "  @staticmethod\n",
        "  def getSquarePiece(piece):\n",
        "    return OthelloGame.square_content[piece]\n",
        "\n",
        "  def __init__(self, n):\n",
        "    self.n = n\n",
        "\n",
        "  def getInitBoard(self):\n",
        "    # return initial board (numpy board)\n",
        "    b = Board(self.n)\n",
        "    return np.array(b.pieces)\n",
        "\n",
        "  def getBoardSize(self):\n",
        "    # (a,b) tuple\n",
        "    return (self.n, self.n)\n",
        "\n",
        "  def getActionSize(self):\n",
        "    # return number of actions, n is the board size and +1 is for no-op action\n",
        "    return self.n*self.n + 1\n",
        "\n",
        "  def getNextState(self, board, player, action):\n",
        "    # if player takes action on board, return next (board,player)\n",
        "    # action must be a valid move\n",
        "    if action == self.n*self.n:\n",
        "      return (board, -player)\n",
        "    b = Board(self.n)\n",
        "    b.pieces = np.copy(board)\n",
        "    move = (int(action/self.n), action%self.n)\n",
        "    b.execute_move(move, player)\n",
        "    return (b.pieces, -player)\n",
        "\n",
        "  def getValidMoves(self, board, player):\n",
        "    # return a fixed size binary vector\n",
        "    valids = [0]*self.getActionSize()\n",
        "    b = Board(self.n)\n",
        "    b.pieces = np.copy(board)\n",
        "    legalMoves =  b.get_legal_moves(player)\n",
        "    if len(legalMoves)==0:\n",
        "      valids[-1]=1\n",
        "      return np.array(valids)\n",
        "    for x, y in legalMoves:\n",
        "      valids[self.n*x+y]=1\n",
        "    return np.array(valids)\n",
        "\n",
        "  def getGameEnded(self, board, player):\n",
        "    # return 0 if not ended, 1 if player 1 won, -1 if player 1 lost\n",
        "    # player = 1\n",
        "    b = Board(self.n)\n",
        "    b.pieces = np.copy(board)\n",
        "    if b.has_legal_moves(player):\n",
        "      return 0\n",
        "    if b.has_legal_moves(-player):\n",
        "      return 0\n",
        "    if b.countDiff(player) > 0:\n",
        "      return 1\n",
        "    return -1\n",
        "\n",
        "  def getCanonicalForm(self, board, player):\n",
        "    # return state if player==1, else return -state if player==-1\n",
        "    return player*board\n",
        "\n",
        "  def getSymmetries(self, board, pi):\n",
        "    # mirror, rotational\n",
        "    assert(len(pi) == self.n**2+1)  # 1 for pass\n",
        "    pi_board = np.reshape(pi[:-1], (self.n, self.n))\n",
        "    l = []\n",
        "\n",
        "    for i in range(1, 5):\n",
        "      for j in [True, False]:\n",
        "        newB = np.rot90(board, i)\n",
        "        newPi = np.rot90(pi_board, i)\n",
        "        if j:\n",
        "          newB = np.fliplr(newB)\n",
        "          newPi = np.fliplr(newPi)\n",
        "        l += [(newB, list(newPi.ravel()) + [pi[-1]])]\n",
        "    return l\n",
        "\n",
        "  def stringRepresentation(self, board):\n",
        "    return board.tobytes()\n",
        "\n",
        "  def stringRepresentationReadable(self, board):\n",
        "    board_s = \"\".join(self.square_content[square] for row in board for square in row)\n",
        "    return board_s\n",
        "\n",
        "  def getScore(self, board, player):\n",
        "    b = Board(self.n)\n",
        "    b.pieces = np.copy(board)\n",
        "    return b.countDiff(player)\n",
        "\n",
        "  @staticmethod\n",
        "  def display(board):\n",
        "    n = board.shape[0]\n",
        "    print(\"   \", end=\"\")\n",
        "    for y in range(n):\n",
        "      print(y, end=\" \")\n",
        "    print(\"\")\n",
        "    print(\"-----------------------\")\n",
        "    for y in range(n):\n",
        "      print(y, \"|\", end=\"\")    # print the row #\n",
        "      for x in range(n):\n",
        "        piece = board[y][x]    # get the piece to print\n",
        "        print(OthelloGame.square_content[piece], end=\" \")\n",
        "      print(\"|\")\n",
        "    print(\"-----------------------\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "9cARn1fDgWgU"
      },
      "source": [
        "## Section 1.1: Create a random player"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "lKiVJw5SgWgU"
      },
      "source": [
        "### Coding Exercise 1.1: Implement a random player"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "avcoGDy-gWgV"
      },
      "source": [
        "class RandomPlayer():\n",
        "  def __init__(self, game):\n",
        "    self.game = game\n",
        "\n",
        "  def play(self, board):\n",
        "\n",
        "    #################################################\n",
        "    ## TODO for students: ##\n",
        "    ## 1. Please compute the valid moves using getValidMoves(). ##\n",
        "    ## 2. Compute the probability over actions.##\n",
        "    ## 3. Pick a random action based on the probability computed above.##\n",
        "    # Fill out function and remove ##\n",
        "    #raise NotImplementedError(\"Implement the random player\")\n",
        "    #################################################\n",
        "\n",
        "    valids = self.game.getValidMoves(board,1) #pass in board and 1 for player\n",
        "    prob = valids/valids.sum()\n",
        "    a = np.random.choice(self.game.getActionSize(), p=prob)\n",
        "\n",
        "    return a\n",
        "\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Coding Exercise 1.1: Implement a random player')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "kh5abH6WgWgV"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_7474bcfc.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "2tpSXbY2gWgV"
      },
      "source": [
        "## Section 1.2. Initiate the game board\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "XhAZYcufgWgV",
        "outputId": "0896d26c-2112-45e8-beda-3ab5fdb76204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Display the board\n",
        "set_seed(seed=SEED)\n",
        "game = OthelloGame(6)\n",
        "board = game.getInitBoard()\n",
        "game.display(board)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random seed 2021 has been set.\n",
            "   0 1 2 3 4 5 \n",
            "-----------------------\n",
            "0 |- - - - - - |\n",
            "1 |- - - - - - |\n",
            "2 |- - X O - - |\n",
            "3 |- - O X - - |\n",
            "4 |- - - - - - |\n",
            "5 |- - - - - - |\n",
            "-----------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "vIHlQv9SgWgV",
        "outputId": "ee2ce95c-6441-406b-e23f-9c4828c30f34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# observe the game board size\n",
        "print(f'Board size = {game.getBoardSize()}')\n",
        "\n",
        "# observe the action size\n",
        "print(f'Action size = {game.getActionSize()}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Board size = (6, 6)\n",
            "Action size = 37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "UWymOdGigWgW"
      },
      "source": [
        "## Section 1.3. Create two random agents to play against each other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "cuudjW99gWgW",
        "outputId": "ed4096bb-7b46-496f-a4a8-37d01fe6ab0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# define the random player\n",
        "player1 = RandomPlayer(game).play  # player 1 is a random player\n",
        "player2 = RandomPlayer(game).play  # player 2 is a random player\n",
        "\n",
        "# define number of games\n",
        "num_games = 20\n",
        "\n",
        "# start the competition\n",
        "set_seed(seed=SEED)\n",
        "arena = Arena.Arena(player1, player2 , game, display=None)  # to see the steps of the competition set \"display=OthelloGame.display\"\n",
        "result = arena.playGames(num_games, verbose=False)  # return  ( number of games won by player1, num of games won by player2, num of games won by nobody)\n",
        "print(f\"\\n\\n{result}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random seed 2021 has been set.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Arena.playGames (1): 100%|██████████| 10/10 [00:00<00:00, 25.68it/s]\n",
            "Arena.playGames (2): 100%|██████████| 10/10 [00:00<00:00, 23.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "(11, 9, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ewyP8d4cgWgW"
      },
      "source": [
        "```\n",
        "(11, 9, 0)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "kGdqnyySgWgW"
      },
      "source": [
        "## Section 1.4. Compute win rate for the random player (player 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "zWRggRY1gWgW",
        "outputId": "31eaaefb-28e8-4f7b-92e2-02cd14e755e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"Number of games won by player1 = {result[0]}, \"\n",
        "      f\"Number of games won by player2 = {result[1]} out of {num_games} games\")\n",
        "win_rate_player1 = result[0]/num_games\n",
        "print(f\"\\nWin rate for player1 over 20 games: {round(win_rate_player1*100, 1)}%\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of games won by player1 = 11, Number of games won by player2 = 9 out of 20 games\n",
            "\n",
            "Win rate for player1 over 20 games: 55.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "i84pQQ8_gWgW"
      },
      "source": [
        "```\n",
        "Number of games won by player1 = 11, Number of games won by player2 = 9 out of 20 games\n",
        "\n",
        "Win rate for player1 over 20 games: 55.0%\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "5j0SpevsgWgX"
      },
      "source": [
        "---\n",
        "# Section 2: Train a value function from expert game data\n",
        "\n",
        "*Time estimate: ~25mins*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "WYFNvb6rgWgX"
      },
      "source": [
        "**Goal:** Learn how to train a value function from a dataset of games played by an expert.\n",
        "\n",
        "**Exercise:** \n",
        "\n",
        "* Load a dataset of expert generated games.\n",
        "* Train a network to minimize MSE for win/loss predictions given board states sampled throughout the game. This will be done on a very small number of games. We will provide a network trained on a larger dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "3o011sh9gWgX"
      },
      "source": [
        "# @title Video 2: Train a value function\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1pg411j7f7\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"f9lZq0WQJFg\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Video 2: Train a value function')\n",
        "\n",
        "display(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "6Ubz817ngWgX"
      },
      "source": [
        "## Section 2.1. Load expert data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "bWEfWgfEgWgX"
      },
      "source": [
        "def loadTrainExamples(folder, filename):\n",
        "  trainExamplesHistory = []\n",
        "  modelFile = os.path.join(folder, filename)\n",
        "  examplesFile = modelFile + \".examples\"\n",
        "  if not os.path.isfile(examplesFile):\n",
        "    print(f'File \"{examplesFile}\" with trainExamples not found!')\n",
        "    r = input(\"Continue? [y|n]\")\n",
        "    if r != \"y\":\n",
        "      sys.exit()\n",
        "  else:\n",
        "    print(\"File with train examples found. Loading it...\")\n",
        "    with open(examplesFile, \"rb\") as f:\n",
        "      trainExamplesHistory = Unpickler(f).load()\n",
        "    print('Loading done!')\n",
        "    # examples based on the model were already collected (loaded)\n",
        "    return trainExamplesHistory"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "9Du2l75YgWgX",
        "outputId": "da371710-8162-4154-840d-efcd5bdcc312",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "path = \"nma_rl_games/alpha-zero/pretrained_models/data/\"\n",
        "loaded_games = loadTrainExamples(folder=path, filename='checkpoint_1.pth.tar')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File with train examples found. Loading it...\n",
            "Loading done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "IBhEg9m-gWgY"
      },
      "source": [
        "## Section 2.2. Define the Neural Network Architecture for Othello\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "0xMQQsx3gWgY"
      },
      "source": [
        "### Coding Exercise 2.2: Implement the NN `OthelloNNet` for Othello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "iv2Q-lUHgWgY"
      },
      "source": [
        "class OthelloNNet(nn.Module):\n",
        "  def __init__(self, game, args):\n",
        "    # game params\n",
        "    self.board_x, self.board_y = game.getBoardSize()\n",
        "    self.action_size = game.getActionSize()\n",
        "    self.args = args\n",
        "\n",
        "    super(OthelloNNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1,\n",
        "                           padding=1)\n",
        "    self.conv3 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n",
        "    self.conv4 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n",
        "\n",
        "    self.bn1 = nn.BatchNorm2d(args.num_channels)\n",
        "    self.bn2 = nn.BatchNorm2d(args.num_channels)\n",
        "    self.bn3 = nn.BatchNorm2d(args.num_channels)\n",
        "    self.bn4 = nn.BatchNorm2d(args.num_channels)\n",
        "\n",
        "    self.fc1 = nn.Linear(args.num_channels * (self.board_x - 4) * (self.board_y - 4), 1024)\n",
        "    self.fc_bn1 = nn.BatchNorm1d(1024)\n",
        "\n",
        "    self.fc2 = nn.Linear(1024, 512)\n",
        "    self.fc_bn2 = nn.BatchNorm1d(512)\n",
        "\n",
        "    self.fc3 = nn.Linear(512, self.action_size)\n",
        "\n",
        "    self.fc4 = nn.Linear(512, 1)\n",
        "\n",
        "  def forward(self, s):\n",
        "    # s: batch_size x board_x x board_y\n",
        "    s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n",
        "    s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n",
        "    s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n",
        "    s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n",
        "    s = F.relu(self.bn4(self.conv4(s)))                          # batch_size x num_channels x (board_x-4) x (board_y-4)\n",
        "    s = s.view(-1, self.args.num_channels * (self.board_x - 4) * (self.board_y - 4))\n",
        "\n",
        "    s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.args.dropout, training=self.training)  # batch_size x 1024\n",
        "    s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.args.dropout, training=self.training)  # batch_size x 512\n",
        "\n",
        "    pi = self.fc3(s)  # batch_size x action_size\n",
        "    v = self.fc4(s)   # batch_size x 1\n",
        "    #################################################\n",
        "    ## TODO for students: Please compute a probability distribution over 'pi' using log softmax (for numerical stability)\n",
        "    # Fill out function and remove\n",
        "    #raise NotImplementedError(\"Calculate the probability distribution and the value\")\n",
        "    #################################################\n",
        "    # return a probability distribution over actions at the current state and the value of the current state.\n",
        "    return F.log_softmax(pi, dim=1), torch.tanh(v)\n",
        "\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Coding Exercise 2.2: Implement the NN OthelloNNet for Othello')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "uTOc8kKDgWgY"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_7b72acfd.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "NDO10OifgWgY"
      },
      "source": [
        "## Section 2.3. Define the Value network\n",
        " During the training the ground truth will be uploaded from the **MCTS simulations** available at 'checkpoint_x.path.tar.examples'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ii5c-PXPgWgZ"
      },
      "source": [
        "### Coding Exercise 2.3: Implement the `ValueNetwork`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "xK7yUpvegWgZ"
      },
      "source": [
        "class ValueNetwork(NeuralNet):\n",
        "  def __init__(self, game):\n",
        "    self.nnet = OthelloNNet(game, args)\n",
        "    self.board_x, self.board_y = game.getBoardSize()\n",
        "    self.action_size = game.getActionSize()\n",
        "\n",
        "    self.nnet.to(args.device)\n",
        "\n",
        "  def train(self, games):\n",
        "    \"\"\"\n",
        "    examples: list of examples, each example is of form (board, pi, v)\n",
        "    \"\"\"\n",
        "    optimizer = optim.Adam(self.nnet.parameters())\n",
        "    for examples in games:\n",
        "      for epoch in range(args.epochs):\n",
        "        print('EPOCH ::: ' + str(epoch + 1))\n",
        "        self.nnet.train()\n",
        "        v_losses = []   # to store the losses per epoch\n",
        "        batch_count = int(len(examples) / args.batch_size)  # len(examples)=200, batch-size=64, batch_count=3\n",
        "        t = tqdm(range(batch_count), desc='Training Value Network')\n",
        "        for _ in t:\n",
        "          sample_ids = np.random.randint(len(examples), size=args.batch_size)  # read the ground truth information from MCTS simulation using the loaded examples\n",
        "          boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))  # length of boards, pis, vis = 64\n",
        "          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
        "          target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
        "\n",
        "          # predict\n",
        "          boards, target_vs = boards.contiguous().to(args.device), target_vs.contiguous().to(args.device)\n",
        "\n",
        "          #################################################\n",
        "          ## TODO for students:\n",
        "          ## 1. Compute the value predicted by OthelloNNet() ##\n",
        "          ## 2. First implement the loss_v() function below and then use it to update the value loss. ##\n",
        "          # Fill out function and remove\n",
        "          #raise NotImplementedError(\"Compute the output\")\n",
        "          #################################################\n",
        "          # compute output\n",
        "          _, out_v = self.nnet(boards)\n",
        "          l_v = self.loss_v(target_vs,out_v)  # total loss\n",
        "\n",
        "          # record loss\n",
        "          v_losses.append(l_v.item())\n",
        "          t.set_postfix(Loss_v=l_v.item())\n",
        "\n",
        "          # compute gradient and do SGD step\n",
        "          optimizer.zero_grad()\n",
        "          l_v.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "  def predict(self, board):\n",
        "    \"\"\"\n",
        "    board: np array with board\n",
        "    \"\"\"\n",
        "    # timing\n",
        "    start = time.time()\n",
        "\n",
        "    # preparing input\n",
        "    board = torch.FloatTensor(board.astype(np.float64))\n",
        "    board = board.contiguous().to(args.device)\n",
        "    board = board.view(1, self.board_x, self.board_y)\n",
        "    self.nnet.eval()\n",
        "    with torch.no_grad():\n",
        "        _, v = self.nnet(board)\n",
        "    return v.data.cpu().numpy()[0]\n",
        "\n",
        "  def loss_v(self, targets, outputs):\n",
        "    #################################################\n",
        "    ## TODO for students: Please compute Mean squared error and return as output. ##\n",
        "    # Fill out function and remove\n",
        "    #raise NotImplementedError(\"Calculate the loss\")\n",
        "    #################################################\n",
        "    # Mean squared error (MSE)\n",
        "    return torch.sum((targets - outputs.view(-1)) ** 2) / targets.size()[0]\n",
        "\n",
        "  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
        "    filepath = os.path.join(folder, filename)\n",
        "    if not os.path.exists(folder):\n",
        "      print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
        "      os.mkdir(folder)\n",
        "    else:\n",
        "      print(\"Checkpoint Directory exists! \")\n",
        "    torch.save({'state_dict': self.nnet.state_dict(),}, filepath)\n",
        "    print(\"Model saved! \")\n",
        "\n",
        "  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
        "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
        "    filepath = os.path.join(folder, filename)\n",
        "    if not os.path.exists(filepath):\n",
        "      raise (\"No model in path {}\".format(filepath))\n",
        "\n",
        "    checkpoint = torch.load(filepath, map_location=args.device)\n",
        "    self.nnet.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Coding Exercise 2.3: Implement the ValueNetwork')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "gvhupD6sgWgZ"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_5f1dc256.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "VGrDVAkWgWgZ"
      },
      "source": [
        "## Section 2.4. Train the value network and observe the MSE loss progress\n",
        "\n",
        "**Important:** Only run this cell if you do not have access to the pretrained models in the `rl_for_games` repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "U8OA4Z79gWga"
      },
      "source": [
        "if not os.listdir('nma_rl_games/alpha-zero/pretrained_models/models/'):\n",
        "  set_seed(seed=SEED)\n",
        "  game = OthelloGame(6)\n",
        "  vnet = ValueNetwork(game)\n",
        "  vnet.train(loaded_games)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "6BKagfmKgWga"
      },
      "source": [
        "---\n",
        "# Section 3: Use a trained value network to play games\n",
        "\n",
        "*Time estimate: ~25mins*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "oqJEs6yhgWga"
      },
      "source": [
        "**Goal**: Learn how to use a value function in order to make a player that works better than a random player.\n",
        "\n",
        "**Exercise:**\n",
        "* Sample random valid moves and use the value function to rank them\n",
        "* Choose the best move as the action and play it\n",
        "Show that doing so beats the random player\n",
        "\n",
        "**Hint:** You might need to change the sign of the value based on the player"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "RRjzA7jQgWga"
      },
      "source": [
        "# @title Video 3: Play games using a value function\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1Ug411j7ig\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"tvmzVHPBKKs\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Video 3: Play games using a value function')\n",
        "\n",
        "display(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "KwnMTqHmgWga"
      },
      "source": [
        "## Coding Exercise 3: Value-based player"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "X5qUBAr5gWga",
        "outputId": "724e6d1a-15ce-44e5-89c3-db14bc150dcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_save_name = 'ValueNetwork.pth.tar'\n",
        "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
        "set_seed(seed=SEED)\n",
        "game = OthelloGame(6)\n",
        "vnet = ValueNetwork(game)\n",
        "vnet.load_checkpoint(folder=path, filename=model_save_name)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random seed 2021 has been set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "qvQ3tAw-gWga",
        "outputId": "742fc594-86e8-4d31-de09-45cd7162e4d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class ValueBasedPlayer():\n",
        "  def __init__(self, game, vnet):\n",
        "    self.game = game\n",
        "    self.vnet = vnet\n",
        "\n",
        "  def play(self, board):\n",
        "    valids = self.game.getValidMoves(board, 1)\n",
        "    candidates = []\n",
        "    max_num_actions = 4\n",
        "    va = np.where(valids)[0]\n",
        "    va_list = va.tolist()\n",
        "    random.shuffle(va_list)\n",
        "    #################################################\n",
        "    ## TODO for students: In the first part, please return the next board state using getNextState(), then predict\n",
        "    ## the value of next state using value network, and finally add the value and action as a tuple to the candidate list.\n",
        "    ## Note that you need to reverse the sign of the value. In zero-sum games the players flip every turn. In detail, we train\n",
        "    ## a value function to think about the game from one player's (either black or white) perspective. In order to use the same\n",
        "    ## value function to estimate how good the position is for the other player, we need to take the negative of the output of\n",
        "    ## the function. E.g., if the value function is trained for white's perspective and says that white is likely to win the game\n",
        "    ## from the current state with an output of 0.75, this similarly means that it would suggest that black is very unlikely (-0.75)\n",
        "    ## to win the game from the current state.##\n",
        "    # Fill out function and remove\n",
        "    #raise NotImplementedError(\"Implement the value-based player\")\n",
        "    #################################################\n",
        "    for a in va_list:\n",
        "      nextBoard, _ = self.game.getNextState(board,1,a) # return next board state using getNextState() function\n",
        "      value = self.vnet.predict(nextBoard) # predict the value of next state using value network\n",
        "      candidates += [(-value,a)] # add the value and the action as a tuple to the candidate lists, note that you might need to change the sign of the value based on the player\n",
        "\n",
        "      if len(candidates) == max_num_actions:\n",
        "        break\n",
        "\n",
        "    candidates.sort()\n",
        "\n",
        "    return candidates[0][1]\n",
        "\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Coding Exercise 3: Value-based player')\n",
        "\n",
        "# playing games between a value-based player and a random player\n",
        "set_seed(seed=SEED)\n",
        "num_games = 20\n",
        "player1 = ValueBasedPlayer(game, vnet).play\n",
        "player2 = RandomPlayer(game).play\n",
        "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
        "## Uncomment the code below to check your code!\n",
        "result = arena.playGames(num_games, verbose=False)\n",
        "print(f\"\\n\\n{result}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random seed 2021 has been set.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Arena.playGames (1): 100%|██████████| 10/10 [00:01<00:00,  5.88it/s]\n",
            "Arena.playGames (2): 100%|██████████| 10/10 [00:01<00:00,  6.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "(14, 6, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "NNBa_l63gWgb"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_45ffeae9.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "hTy6JMeHgWgb"
      },
      "source": [
        "```\n",
        "(14, 6, 0)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "s0OyHImYgWgb"
      },
      "source": [
        "**Result of pitting a value-based player against a random player**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "JbAPK7-fgWgb",
        "outputId": "5f5d8726-108e-4340-d24d-f95f38876fcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"Number of games won by player1 = {result[0]}, \"\n",
        "      f\"Number of games won by player2 = {result[1]}, out of {num_games} games\")\n",
        "win_rate_player1 = result[0]/num_games # result[0] is the number of times that player 1 wins\n",
        "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of games won by player1 = 14, Number of games won by player2 = 6, out of 20 games\n",
            "\n",
            "Win rate for player1 over 20 games: 70.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "txc-VWGKgWgb"
      },
      "source": [
        "```\n",
        "Number of games won by player1 = 14, Number of games won by player2 = 6, out of 20 games\n",
        "\n",
        "Win rate for player1 over 20 games: 70.0%\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Bp1fpSVJgWgc"
      },
      "source": [
        "---\n",
        "# Section 4: Train a policy network from expert game data\n",
        "\n",
        "*Time estimate: ~20mins*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "_5eJ9H-rgWgc"
      },
      "source": [
        "**Goal**: How to train a policy network via supervised learning / behavioural cloning.\n",
        "\n",
        "**Exercise**:\n",
        "* Train a network to predict the next move in an expert dataset by maximizing the log likelihood of the next action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "FlKZEtF9gWgc"
      },
      "source": [
        "# @title Video 4: Train a policy network\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1hQ4y127GJ\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"vj9gKNJ19D8\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Video 4: Train a policy network')\n",
        "\n",
        "display(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "pHL6VKfsgWgc"
      },
      "source": [
        "## Coding Exercise 4: Implement `PolicyNetwork`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "9j8erzBfgWgc"
      },
      "source": [
        "class PolicyNetwork(NeuralNet):\n",
        "  def __init__(self, game):\n",
        "    self.nnet = OthelloNNet(game, args)\n",
        "    self.board_x, self.board_y = game.getBoardSize()\n",
        "    self.action_size = game.getActionSize()\n",
        "\n",
        "    self.nnet.to(args.device)\n",
        "\n",
        "  def train(self, games):\n",
        "    \"\"\"\n",
        "    examples: list of examples, each example is of form (board, pi, v)\n",
        "    \"\"\"\n",
        "    optimizer = optim.Adam(self.nnet.parameters())\n",
        "\n",
        "    for examples in games:\n",
        "      for epoch in range(args.epochs):\n",
        "        print('EPOCH ::: ' + str(epoch + 1))\n",
        "        self.nnet.train()\n",
        "        pi_losses = []\n",
        "\n",
        "        batch_count = int(len(examples) / args.batch_size)\n",
        "\n",
        "        t = tqdm(range(batch_count), desc='Training Policy Network')\n",
        "        for _ in t:\n",
        "          sample_ids = np.random.randint(len(examples), size=args.batch_size)\n",
        "          boards, pis, _ = list(zip(*[examples[i] for i in sample_ids]))\n",
        "          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
        "          target_pis = torch.FloatTensor(np.array(pis))\n",
        "\n",
        "          # predict\n",
        "          boards, target_pis = boards.contiguous().to(args.device), target_pis.contiguous().to(args.device)\n",
        "\n",
        "          #################################################\n",
        "          ## TODO for students: ##\n",
        "          ## 1. Compute the policy (pi) predicted by OthelloNNet() ##\n",
        "          ## 2. Implement the loss_pi() function below and then use it to update the policy loss. ##\n",
        "          # Fill out function and remove\n",
        "          raise NotImplementedError(\"Compute the output\")\n",
        "          #################################################\n",
        "          # compute output\n",
        "          out_pi, _ = ...\n",
        "          l_pi = ...\n",
        "\n",
        "          # record loss\n",
        "          pi_losses.append(l_pi.item())\n",
        "          t.set_postfix(Loss_pi=l_pi.item())\n",
        "\n",
        "          # compute gradient and do SGD step\n",
        "          optimizer.zero_grad()\n",
        "          l_pi.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "  def predict(self, board):\n",
        "    \"\"\"\n",
        "    board: np array with board\n",
        "    \"\"\"\n",
        "    # timing\n",
        "    start = time.time()\n",
        "\n",
        "    # preparing input\n",
        "    board = torch.FloatTensor(board.astype(np.float64))\n",
        "    board = board.contiguous().to(args.device)\n",
        "    board = board.view(1, self.board_x, self.board_y)\n",
        "    self.nnet.eval()\n",
        "    with torch.no_grad():\n",
        "      pi,_ = self.nnet(board)\n",
        "    return torch.exp(pi).data.cpu().numpy()[0]\n",
        "\n",
        "  def loss_pi(self, targets, outputs):\n",
        "    #################################################\n",
        "    ## TODO for students: To implement the loss function, please compute and return the negative log likelihood of targets.\n",
        "    ## For more information, here is a reference that connects the expression to the neg-log-prob: https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
        "    # Fill out function and remove\n",
        "    raise NotImplementedError(\"Compute the loss\")\n",
        "    #################################################\n",
        "    return ...\n",
        "\n",
        "  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
        "    filepath = os.path.join(folder, filename)\n",
        "    if not os.path.exists(folder):\n",
        "      print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
        "      os.mkdir(folder)\n",
        "    else:\n",
        "      print(\"Checkpoint Directory exists! \")\n",
        "    torch.save({'state_dict': self.nnet.state_dict(),}, filepath)\n",
        "    print(\"Model saved! \")\n",
        "\n",
        "  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
        "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
        "    filepath = os.path.join(folder, filename)\n",
        "    if not os.path.exists(filepath):\n",
        "      raise (\"No model in path {}\".format(filepath))\n",
        "\n",
        "    checkpoint = torch.load(filepath, map_location=args.device)\n",
        "    self.nnet.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Coding Exercise 4: Implement PolicyNetwork')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rX9tpRTKgWgd"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_fe4292a6.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "n4B5uTIWgWgd"
      },
      "source": [
        "### Train the policy network\n",
        "\n",
        "**Important:** Only run this cell if you do not have access to the pretrained models in the `rl_for_games` repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "HyW8ySgfgWgd"
      },
      "source": [
        "if not os.listdir('nma_rl_games/alpha-zero/pretrained_models/models/'):\n",
        "  set_seed(seed=SEED)\n",
        "  game = OthelloGame(6)\n",
        "  pnet = PolicyNetwork(game)\n",
        "  pnet.train(loaded_games)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "QMJz0uhbgWge"
      },
      "source": [
        "---\n",
        "# Section 5: Use a trained policy network to play games\n",
        "\n",
        "Time estimate: ~20mins\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "p8KlqzXSgWge"
      },
      "source": [
        "**Goal**: How to use a policy network to play games.\n",
        "\n",
        "**Exercise:** \n",
        "* Use the policy network to give probabilities for the next move.\n",
        "* Build a player that takes the move given the maximum probability by the network.\n",
        "* Compare this to another player that samples moves according to the probability distribution output by the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "XUnhTWpYgWge"
      },
      "source": [
        "# @title Video 5: Play games using a policy network\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1aq4y1S7o4\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"yHtVqT2Nstk\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Video 5: Play games using a policy network')\n",
        "\n",
        "display(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "OrGYOihhgWge"
      },
      "source": [
        "## Coding Exercise 5: Implement the `PolicyBasedPlayer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "InAnB1ChgWge"
      },
      "source": [
        "model_save_name = 'PolicyNetwork.pth.tar'\n",
        "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
        "set_seed(seed=SEED)\n",
        "game = OthelloGame(6)\n",
        "pnet = PolicyNetwork(game)\n",
        "pnet.load_checkpoint(folder=path, filename=model_save_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "QRG1Hr67gWge"
      },
      "source": [
        "class PolicyBasedPlayer():\n",
        "  def __init__(self, game, pnet, greedy=True):\n",
        "    self.game = game\n",
        "    self.pnet = pnet\n",
        "    self.greedy = greedy\n",
        "\n",
        "  def play(self, board):\n",
        "    valids = self.game.getValidMoves(board, 1)\n",
        "    #################################################\n",
        "    ## TODO for students:  ##\n",
        "    ## 1. Compute the action probabilities using policy network pnet()\n",
        "    ## 2. Mask invalid moves using valids variable and the action probabilites computed above.\n",
        "    ## 3. Compute the sum over valid actions and store them in sum_vap.\n",
        "    # Fill out function and remove\n",
        "    raise NotImplementedError(\"Define the play\")\n",
        "    #################################################\n",
        "    action_probs = ...\n",
        "    vap = ...  # masking invalid moves\n",
        "    sum_vap = ...\n",
        "\n",
        "    if sum_vap > 0:\n",
        "      vap /= sum_vap  # renormalize\n",
        "    else:\n",
        "      # if all valid moves were masked we make all valid moves equally probable\n",
        "      print(\"All valid moves were masked, doing a workaround.\")\n",
        "      vap = vap + valids\n",
        "      vap /= np.sum(vap)\n",
        "\n",
        "    if self.greedy:\n",
        "      # greedy policy player\n",
        "      a = np.where(vap == np.max(vap))[0][0]\n",
        "    else:\n",
        "      # sample-based policy player\n",
        "      a = np.random.choice(self.game.getActionSize(), p=vap)\n",
        "\n",
        "    return a\n",
        "\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Coding Exercise 5: Implement the PolicyBasedPlayer')\n",
        "\n",
        "# playing games\n",
        "set_seed(seed=SEED)\n",
        "num_games = 20\n",
        "player1 = PolicyBasedPlayer(game, pnet, greedy=True).play\n",
        "player2 = RandomPlayer(game).play\n",
        "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
        "## Uncomment below to test!\n",
        "# result = arena.playGames(num_games, verbose=False)\n",
        "# print(f\"\\n\\n{result}\")\n",
        "# win_rate_player1 = result[0] / num_games\n",
        "# print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jdCAEwLpgWgf"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_ef26beca.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ckjUMiWcgWgf"
      },
      "source": [
        "```\n",
        " Win rate for player1 over 20 games: 80.0%\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "tLrmC3-9gWgf"
      },
      "source": [
        "### Comparing a policy based player versus a random player\n",
        "\n",
        "There's often randomness in the results as we are running the players for a low number of games (only 20 games due compute + time costs). So, when students are running the cells they might not get the expected result. To better measure the strength of players you can run more games!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "fT_MRTqGgWgf"
      },
      "source": [
        "set_seed(seed=SEED)\n",
        "num_games = 20\n",
        "game = OthelloGame(6)\n",
        "player1 = PolicyBasedPlayer(game, pnet, greedy=False).play\n",
        "player2 = RandomPlayer(game).play\n",
        "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
        "result = arena.playGames(num_games, verbose=False)\n",
        "print(f\"\\n\\n{result}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "vKcqj6TIgWgf"
      },
      "source": [
        "win_rate_player1 = result[0]/num_games\n",
        "print(f\"Win rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "0mjFeZl2gWgf"
      },
      "source": [
        "```\n",
        "Win rate for player1 over 20 games: 95.0%\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "XKL7l_jhgWgf"
      },
      "source": [
        "### Compare greedy policy based player versus value based player "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "jnG0TXnsgWgg"
      },
      "source": [
        "set_seed(seed=SEED)\n",
        "num_games = 20\n",
        "game = OthelloGame(6)\n",
        "player1 = PolicyBasedPlayer(game, pnet).play\n",
        "player2 = ValueBasedPlayer(game, vnet).play\n",
        "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
        "result = arena.playGames(num_games, verbose=False)\n",
        "print(f\"\\n\\n{result}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "5wPKKs4AgWgg"
      },
      "source": [
        "win_rate_player1 = result[0]/num_games\n",
        "print(f\"Win rate for player 1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "d0_C3yYrgWgg"
      },
      "source": [
        "```\n",
        "Win rate for player 1 over 20 games: 55.0%\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "OgIGfonjgWgg"
      },
      "source": [
        "### Compare greedy policy based player versus sample-based policy player "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "VS8eYUxagWgg"
      },
      "source": [
        "set_seed(seed=SEED)\n",
        "num_games = 20\n",
        "game = OthelloGame(6)\n",
        "player1 = PolicyBasedPlayer(game, pnet).play # greedy player\n",
        "player2 = PolicyBasedPlayer(game, pnet, greedy=False).play # sample-based player\n",
        "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
        "result = arena.playGames(num_games, verbose=False)\n",
        "print(f\"\\n\\n{result}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "DbiuwQUtgWgg"
      },
      "source": [
        "win_rate_player1 = result[0]/num_games\n",
        "print(f\"Win rate for player 1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "zgwilLIlgWgg"
      },
      "source": [
        "```\n",
        " Win rate for player 1 over 20 games: 50.0%\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "SN_cMbI8gWgg"
      },
      "source": [
        "---\n",
        "# Section 6: Plan using Monte Carlo rollouts\n",
        "\n",
        "*Time estimate: ~15mins*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Ho0LclqhgWgg"
      },
      "source": [
        "**Goal**: Teach the students the core idea behind using simulated rollouts to understand the future and value actions.\n",
        "\n",
        "**Exercise**:\n",
        "* Build a loop to run Monte Carlo simulations using the policy network.\n",
        "* Use this to obtain better estimates of the value of moves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "9JuF_mr1gWgh"
      },
      "source": [
        "# @title Video 6: Play using Monte-Carlo rollouts\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1Rb4y1U7BW\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"DtCWDIlSo18\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Video 6: Play using Monte-Carlo rollouts')\n",
        "\n",
        "display(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "8tvCb_mKgWgh"
      },
      "source": [
        "## Coding Exercise 6: `MonteCarlo`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "rasDKZ0tgWgh"
      },
      "source": [
        "class MonteCarlo():\n",
        "  def __init__(self, game, nnet, args):\n",
        "    self.game = game\n",
        "    self.nnet = nnet\n",
        "    self.args = args\n",
        "\n",
        "    self.Ps = {}  # stores initial policy (returned by neural net)\n",
        "    self.Es = {}  # stores game.getGameEnded ended for board s\n",
        "\n",
        "  # call this rollout\n",
        "  def simulate(self, canonicalBoard):\n",
        "    \"\"\"\n",
        "    This function performs one monte carlo rollout\n",
        "    \"\"\"\n",
        "\n",
        "    s = self.game.stringRepresentation(canonicalBoard)\n",
        "    init_start_state = s\n",
        "    temp_v = 0\n",
        "    isfirstAction = None\n",
        "\n",
        "    for i in range(self.args.maxDepth): # maxDepth\n",
        "\n",
        "      if s not in self.Es:\n",
        "        self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
        "      if self.Es[s] != 0:\n",
        "        # terminal state\n",
        "        temp_v= -self.Es[s]\n",
        "        break\n",
        "\n",
        "      self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
        "      valids = self.game.getValidMoves(canonicalBoard, 1)\n",
        "      self.Ps[s] = self.Ps[s] * valids  # masking invalid moves\n",
        "      sum_Ps_s = np.sum(self.Ps[s])\n",
        "\n",
        "      if sum_Ps_s > 0:\n",
        "        self.Ps[s] /= sum_Ps_s  # renormalize\n",
        "      else:\n",
        "        # if all valid moves were masked make all valid moves equally probable\n",
        "        # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
        "        # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.\n",
        "        log.error(\"All valid moves were masked, doing a workaround.\")\n",
        "        self.Ps[s] = self.Ps[s] + valids\n",
        "        self.Ps[s] /= np.sum(self.Ps[s])\n",
        "\n",
        "      #################################################\n",
        "      ## TODO for students: Take a random action.\n",
        "      ## 1. Take the random action.\n",
        "      ## 2. Find the next state and the next player from the environment.\n",
        "      ## 3. Get the canonical form of the next state.\n",
        "      # Fill out function and remove\n",
        "      raise NotImplementedError(\"Take the action, find the next state\")\n",
        "      #################################################\n",
        "      a = ...\n",
        "      next_s, next_player = self.game.getNextState(..., ..., ...)\n",
        "      next_s = self.game.getCanonicalForm(..., ...)\n",
        "\n",
        "      s = self.game.stringRepresentation(next_s)\n",
        "      temp_v = v\n",
        "\n",
        "    return temp_v\n",
        "\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Coding Exercise 6: MonteCarlo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "fKL2FE8bgWgh"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_d8a53ceb.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "CE_qXIdzgWgh"
      },
      "source": [
        "---\n",
        "# Section 7: Use Monte Carlo simulations to play games\n",
        "\n",
        "*Time estimate: ~20mins*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Sa_pszwQgWgh"
      },
      "source": [
        "**Goal:** Teach students how to use simple Monte Carlo planning to play games."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "AYxHRMMPgWgi"
      },
      "source": [
        "# @title Video 7: Play with planning\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1bh411B7S4\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"plmFzAy3H5s\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Video 7: Play with planning')\n",
        "\n",
        "display(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "zNH6kCV4gWgi"
      },
      "source": [
        "## Coding Exercise 7: Monte-Carlo simulations\n",
        "\n",
        "* Incorporate Monte Carlo simulations into an agent.\n",
        "* Run the resulting player versus the random, value-based, and policy-based players."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "r_AvzhMpgWgi"
      },
      "source": [
        "# Load MC model from the repository\n",
        "mc_model_save_name = 'MC.pth.tar'\n",
        "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "nHnBWNhNgWgi"
      },
      "source": [
        "class MonteCarloBasedPlayer():\n",
        "  def __init__(self, game, nnet, args):\n",
        "    self.game = game\n",
        "    self.nnet = nnet\n",
        "    self.args = args\n",
        "    #################################################\n",
        "    ## TODO for students: Instantiate the Monte Carlo class.\n",
        "    # Fill out function and remove\n",
        "    raise NotImplementedError(\"Use Monte Carlo!\")\n",
        "    #################################################\n",
        "    self.mc = ...\n",
        "    self.K = self.args.mc_topk\n",
        "\n",
        "  def play(self, canonicalBoard):\n",
        "    self.qsa = []\n",
        "    s = self.game.stringRepresentation(canonicalBoard)\n",
        "    Ps, v = self.nnet.predict(canonicalBoard)\n",
        "    valids = self.game.getValidMoves(canonicalBoard, 1)\n",
        "    Ps = Ps * valids  # masking invalid moves\n",
        "    sum_Ps_s = np.sum(Ps)\n",
        "\n",
        "    if sum_Ps_s > 0:\n",
        "      Ps /= sum_Ps_s  # renormalize\n",
        "    else:\n",
        "      # if all valid moves were masked make all valid moves equally probable\n",
        "      # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
        "      # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.\n",
        "      log = logging.getLogger(__name__)\n",
        "      log.error(\"All valid moves were masked, doing a workaround.\")\n",
        "      Ps = Ps + valids\n",
        "      Ps /= np.sum(Ps)\n",
        "\n",
        "    num_valid_actions = np.shape(np.nonzero(Ps))[1]\n",
        "\n",
        "    if num_valid_actions < self.K:\n",
        "      top_k_actions = np.argpartition(Ps,-num_valid_actions)[-num_valid_actions:]\n",
        "    else:\n",
        "      top_k_actions = np.argpartition(Ps,-self.K)[-self.K:]  # to get actions that belongs to top k prob\n",
        "    #################################################\n",
        "    ## TODO for students:\n",
        "    ## 1. For each action in the top-k actions\n",
        "    ## 2. Get the next state using getNextState() function. You can find the implementation of this function in Section 1 in OthelloGame() class.\n",
        "    ## 3. Get the canonical form of the getNextState().\n",
        "    # Fill out function and remove\n",
        "    raise NotImplementedError(\"Loop for the top actions\")\n",
        "    #################################################\n",
        "    for action in ...:\n",
        "      next_s, next_player = self.game.getNextState(..., ..., ...)\n",
        "      next_s = self.game.getCanonicalForm(..., ...)\n",
        "\n",
        "      values = []\n",
        "\n",
        "      # do some rollouts\n",
        "      for rollout in range(self.args.numMCsims):\n",
        "        value = self.mc.simulate(canonicalBoard)\n",
        "        values.append(value)\n",
        "\n",
        "      # average out values\n",
        "      avg_value = np.mean(values)\n",
        "      self.qsa.append((avg_value, action))\n",
        "\n",
        "    self.qsa.sort(key=lambda a: a[0])\n",
        "    self.qsa.reverse()\n",
        "    best_action = self.qsa[0][1]\n",
        "    return best_action\n",
        "\n",
        "  def getActionProb(self, canonicalBoard, temp=1):\n",
        "    if self.game.getGameEnded(canonicalBoard, 1) != 0:\n",
        "      return np.zeros((self.game.getActionSize()))\n",
        "\n",
        "    else:\n",
        "      action_probs = np.zeros((self.game.getActionSize()))\n",
        "      best_action = self.play(canonicalBoard)\n",
        "      action_probs[best_action] = 1\n",
        "\n",
        "    return action_probs\n",
        "\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Coding Exercise 7: MonteCarlo siumulations')\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "game = OthelloGame(6)\n",
        "# Run the resulting player versus the random player\n",
        "rp = RandomPlayer(game).play\n",
        "num_games = 20  # Feel free to change this number\n",
        "\n",
        "n1 = NNet(game)  # nNet players\n",
        "n1.load_checkpoint(folder=path, filename=mc_model_save_name)\n",
        "args1 = dotdict({'numMCsims': 10, 'maxRollouts':5, 'maxDepth':5, 'mc_topk': 3})\n",
        "\n",
        "## Uncomment below to check Monte Carlo agent!\n",
        "# print('\\n******MC player versus random player******')\n",
        "# mc1 = MonteCarloBasedPlayer(game, n1, args1)\n",
        "# n1p = lambda x: np.argmax(mc1.getActionProb(x))\n",
        "# arena = Arena.Arena(n1p, rp, game, display=OthelloGame.display)\n",
        "# MC_result = arena.playGames(num_games, verbose=False)\n",
        "# print(f\"\\n\\n{MC_result}\")\n",
        "# print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
        "#       f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
        "# win_rate_player1 = MC_result[0]/num_games\n",
        "# print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Bu2d_Jl6gWgj"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_e774cb2e.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "aI9UqDvogWgj"
      },
      "source": [
        "```\n",
        "Number of games won by player1 = 11, number of games won by player2 = 9, out of 20 games\n",
        "\n",
        "Win rate for player1 over 20 games: 55.0%\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "PxjC5W1KgWgj"
      },
      "source": [
        "### Monte-Carlo player against Value-based player"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "vx9og-gBgWgj"
      },
      "source": [
        "print('\\n******MC player versus value-based player******')\n",
        "set_seed(seed=SEED)\n",
        "vp = ValueBasedPlayer(game, vnet).play # value-based player\n",
        "arena = Arena.Arena(n1p, vp, game, display=OthelloGame.display)\n",
        "MC_result = arena.playGames(num_games, verbose=False)\n",
        "print(f\"\\n\\n{MC_result}\")\n",
        "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
        "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
        "win_rate_player1 = MC_result[0]/num_games\n",
        "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "uTtlscpjgWgj"
      },
      "source": [
        "```\n",
        "Number of games won by player1 = 10, number of games won by player2 = 10, out of 20 games\n",
        "\n",
        "Win rate for player1 over 20 games: 50.0%\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "KgKuP3ewgWgj"
      },
      "source": [
        "### Monte-Carlo player against Policy-based player"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "Hou1wpoCgWgk"
      },
      "source": [
        "print('\\n******MC player versus policy-based player******')\n",
        "set_seed(seed=SEED)\n",
        "pp = PolicyBasedPlayer(game, pnet).play # policy player\n",
        "arena = Arena.Arena(n1p, pp, game, display=OthelloGame.display)\n",
        "MC_result = arena.playGames(num_games, verbose=False)\n",
        "print(f\"\\n\\n{MC_result}\")\n",
        "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
        "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
        "win_rate_player1 = MC_result[0]/num_games\n",
        "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "my4413qSgWgk"
      },
      "source": [
        "```\n",
        "Number of games won by player1 = 10, number of games won by player2 = 10, out of 20 games\n",
        "\n",
        "Win rate for player1 over 20 games: 50.0%\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "CuU8HB0LgWgk"
      },
      "source": [
        "---\n",
        "# Section 8: Ethical aspects\n",
        "\n",
        "*Time estimate: ~5mins*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "A7myg8xegWgk"
      },
      "source": [
        "# @title Video 8: Unstoppable opponents\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1WA411w7mw\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"q7181lvoNpM\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Video 8: Unstoppable opponents')\n",
        "\n",
        "display(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "cuJiZKJVgWgk"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "In this tutotial, you have learned how to implement a game loop and improve the performance of a random player. More specifically, you are now able to understand the format of two-players games. We learned about value-based and policy-based players, and we compare them with the MCTS method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "ghkLRVxGgWgk"
      },
      "source": [
        "# @title Video 9: Outro\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1a64y1s7Sh\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"uQ26iIUzmtw\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Video 9: Outro')\n",
        "\n",
        "display(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "FumMER-_gWgl"
      },
      "source": [
        "# @title Airtable Submission Link\n",
        "from IPython import display as IPydisplay\n",
        "IPydisplay.HTML(\n",
        "   f\"\"\"\n",
        " <div>\n",
        "   <a href= \"{atform.url()}\" target=\"_blank\">\n",
        "   <img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/SurveyButton.png?raw=1\"\n",
        " alt=\"button link end of day Survey\" style=\"width:410px\"></a>\n",
        "   </div>\"\"\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "e85NquyngWgl"
      },
      "source": [
        "---\n",
        "# Bonus 1: Plan using Monte Carlo Tree Search (MCTS)\n",
        "\n",
        "*Time estimate: ~30mins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "bi0AXfalgWgl"
      },
      "source": [
        "**Goal:** Teach students to understand the core ideas behind Monte Carlo Tree Search (MCTS)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "k-HYoKYogWgl"
      },
      "source": [
        "# @title Video 10: Plan with MCTS\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV1yQ4y127Sr\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"Hhw6Ed0Zmco\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Video 10: Plan with MCTS')\n",
        "\n",
        "display(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "5pcEp7r-gWgl"
      },
      "source": [
        "## Bonus Coding Exercise 1: MCTS planner\n",
        "\n",
        "* Plug together pre-built Selection, Expansion & Backpropagation code to complete an MCTS planner.\n",
        "* Deploy the MCTS planner to understand an interesting position, producing value estimates and action counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "I3hkinNBgWgl"
      },
      "source": [
        "class MCTS():\n",
        "  \"\"\"\n",
        "  This class handles the MCTS tree.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, game, nnet, args):\n",
        "    self.game = game\n",
        "    self.nnet = nnet\n",
        "    self.args = args\n",
        "    self.Qsa = {}    # stores Q values for s,a (as defined in the paper)\n",
        "    self.Nsa = {}    # stores #times edge s,a was visited\n",
        "    self.Ns = {}     # stores #times board s was visited\n",
        "    self.Ps = {}     # stores initial policy (returned by neural net)\n",
        "    self.Es = {}     # stores game.getGameEnded ended for board s\n",
        "    self.Vs = {}     # stores game.getValidMoves for board s\n",
        "\n",
        "  def search(self, canonicalBoard):\n",
        "    \"\"\"\n",
        "    This function performs one iteration of MCTS. It is recursively called\n",
        "    till a leaf node is found. The action chosen at each node is one that\n",
        "    has the maximum upper confidence bound as in the paper.\n",
        "\n",
        "    Once a leaf node is found, the neural network is called to return an\n",
        "    initial policy P and a value v for the state. This value is propagated\n",
        "    up the search path. In case the leaf node is a terminal state, the\n",
        "    outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
        "    updated.\n",
        "\n",
        "    NOTE: the return values are the negative of the value of the current\n",
        "    state. This is done since v is in [-1,1] and if v is the value of a\n",
        "    state for the current player, then its value is -v for the other player.\n",
        "\n",
        "    Returns:\n",
        "        v: the negative of the value of the current canonicalBoard\n",
        "    \"\"\"\n",
        "    s = self.game.stringRepresentation(canonicalBoard)\n",
        "\n",
        "    if s not in self.Es:\n",
        "      self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
        "    if self.Es[s] != 0:\n",
        "      # terminal node\n",
        "      return -self.Es[s]\n",
        "\n",
        "    if s not in self.Ps:\n",
        "      # leaf node\n",
        "      self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
        "      valids = self.game.getValidMoves(canonicalBoard, 1)\n",
        "      self.Ps[s] = self.Ps[s] * valids  # masking invalid moves\n",
        "      sum_Ps_s = np.sum(self.Ps[s])\n",
        "      if sum_Ps_s > 0:\n",
        "        self.Ps[s] /= sum_Ps_s  # renormalize\n",
        "      else:\n",
        "        # if all valid moves were masked make all valid moves equally probable\n",
        "        # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
        "        # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.\n",
        "        log = logging.getLogger(__name__)\n",
        "        log.error(\"All valid moves were masked, doing a workaround.\")\n",
        "        self.Ps[s] = self.Ps[s] + valids\n",
        "        self.Ps[s] /= np.sum(self.Ps[s])\n",
        "\n",
        "      self.Vs[s] = valids\n",
        "      self.Ns[s] = 0\n",
        "\n",
        "      return -v\n",
        "\n",
        "    valids = self.Vs[s]\n",
        "    cur_best = -float('inf')\n",
        "    best_act = -1\n",
        "\n",
        "    #################################################\n",
        "    ## TODO for students:\n",
        "    ## Implement the highest upper confidence bound depending whether we observed the state-action pair which is stored in self.Qsa[(s, a)]. You can find the formula in the slide 52 in video 8 above.\n",
        "    # Fill out function and remove\n",
        "    raise NotImplementedError(\"Complete the for loop\")\n",
        "    #################################################\n",
        "    # pick the action with the highest upper confidence bound\n",
        "    for a in range(self.game.getActionSize()):\n",
        "      if valids[a]:\n",
        "        if (s, a) in self.Qsa:\n",
        "          u = ... + ... * ... * math.sqrt(...) / (1 + ...)\n",
        "        else:\n",
        "          u = ... * ... * math.sqrt(... + 1e-8)\n",
        "\n",
        "        if u > cur_best:\n",
        "          cur_best = u\n",
        "          best_act = a\n",
        "\n",
        "    a = best_act\n",
        "    next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
        "    next_s = self.game.getCanonicalForm(next_s, next_player)\n",
        "\n",
        "    v = self.search(next_s)\n",
        "\n",
        "    if (s, a) in self.Qsa:\n",
        "      self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
        "      self.Nsa[(s, a)] += 1\n",
        "\n",
        "    else:\n",
        "      self.Qsa[(s, a)] = v\n",
        "      self.Nsa[(s, a)] = 1\n",
        "\n",
        "    self.Ns[s] += 1\n",
        "    return -v\n",
        "\n",
        "  def getNsa(self):\n",
        "    return self.Nsa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "_KOVrwvjgWgm"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_83be26c4.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "3umW22S3gWgm"
      },
      "source": [
        "---\n",
        "# Bonus 2: Use MCTS to play games\n",
        "\n",
        "*Time estimate: ~10mins*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ceDq4JxogWgm"
      },
      "source": [
        "**Goal:** Teach the students how to use the results of an MCTS to play games.\n",
        "\n",
        "**Exercise:** \n",
        "* Plug the MCTS planner into an agent.\n",
        "* Play games against other agents.\n",
        "* Explore the contributions of prior network, value function, number of simulations / time to play, and explore/exploit parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "CwvzuEPDgWgm"
      },
      "source": [
        "# @title Video 11: Play with MCTS\n",
        "from ipywidgets import widgets\n",
        "\n",
        "out2 = widgets.Output()\n",
        "with out2:\n",
        "  from IPython.display import IFrame\n",
        "  class BiliVideo(IFrame):\n",
        "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
        "      self.id=id\n",
        "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
        "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
        "\n",
        "  video = BiliVideo(id=f\"BV13q4y1H7H6\", width=854, height=480, fs=1)\n",
        "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
        "  display(video)\n",
        "\n",
        "out1 = widgets.Output()\n",
        "with out1:\n",
        "  from IPython.display import YouTubeVideo\n",
        "  video = YouTubeVideo(id=f\"1BRXb-igKAU\", width=854, height=480, fs=1, rel=0)\n",
        "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
        "  display(video)\n",
        "\n",
        "out = widgets.Tab([out1, out2])\n",
        "out.set_title(0, 'Youtube')\n",
        "out.set_title(1, 'Bilibili')\n",
        "\n",
        "# add event to airtable\n",
        "atform.add_event('Video 11: Play with MCTS')\n",
        "\n",
        "display(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "hCyapjcggWgn"
      },
      "source": [
        "## Bonus Coding Exercise 2: Agent that uses an MCTS planner\n",
        "\n",
        "* Plug the MCTS planner into an agent.\n",
        "* Play games against other agents.\n",
        "* Explore the contributions of prior network, value function, number of simulations / time to play, and explore/exploit parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "cEICWoVEgWgn"
      },
      "source": [
        "# Load MCTS model from the repository\n",
        "mcts_model_save_name = 'MCTS.pth.tar'\n",
        "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "9Y0AtanSgWgn"
      },
      "source": [
        "class MonteCarloTreeSearchBasedPlayer():\n",
        "  def __init__(self, game, nnet, args):\n",
        "    self.game = game\n",
        "    self.nnet = nnet\n",
        "    self.args = args\n",
        "    self.mcts = MCTS(game, nnet, args)\n",
        "\n",
        "\n",
        "  def play(self, canonicalBoard, temp=1):\n",
        "    for i in range(self.args.numMCTSSims):\n",
        "\n",
        "      #################################################\n",
        "      ## TODO for students:\n",
        "      #  Run MCTS search function.\n",
        "      #  Fill out function and remove\n",
        "      raise NotImplementedError(\"Plug the planner\")\n",
        "      #################################################\n",
        "      ...\n",
        "\n",
        "    s = self.game.stringRepresentation(canonicalBoard)\n",
        "    #################################################\n",
        "    ## TODO for students:\n",
        "    #  Call the Nsa function from MCTS class and store it in the self.Nsa\n",
        "    #  Fill out function and remove\n",
        "    raise NotImplementedError(\"Compute Nsa (number of times edge s,a was visited)\")\n",
        "    #################################################\n",
        "    self.Nsa = ...\n",
        "    self.counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
        "\n",
        "    if temp == 0:\n",
        "      bestAs = np.array(np.argwhere(self.counts == np.max(self.counts))).flatten()\n",
        "      bestA = np.random.choice(bestAs)\n",
        "      probs = [0] * len(self.counts)\n",
        "      probs[bestA] = 1\n",
        "      return probs\n",
        "\n",
        "    self.counts = [x ** (1. / temp) for x in self.counts]\n",
        "    self.counts_sum = float(sum(self.counts))\n",
        "    probs = [x / self.counts_sum for x in self.counts]\n",
        "    return np.argmax(probs)\n",
        "\n",
        "  def getActionProb(self, canonicalBoard, temp=1):\n",
        "    action_probs = np.zeros((self.game.getActionSize()))\n",
        "    best_action = self.play(canonicalBoard)\n",
        "    action_probs[best_action] = 1\n",
        "\n",
        "    return action_probs\n",
        "\n",
        "\n",
        "set_seed(seed=SEED)\n",
        "game = OthelloGame(6)\n",
        "rp = RandomPlayer(game).play  # all players\n",
        "num_games = 20  # games\n",
        "n1 = NNet(game)  # nnet players\n",
        "n1.load_checkpoint(folder=path, filename=mcts_model_save_name)\n",
        "args1 = dotdict({'numMCTSSims': 50, 'cpuct':1.0})\n",
        "\n",
        "## Uncomment below to check your agent!\n",
        "# print('\\n******MCTS player versus random player******')\n",
        "# mcts1 = MonteCarloTreeSearchBasedPlayer(game, n1, args1)\n",
        "# n1p = lambda x: np.argmax(mcts1.getActionProb(x, temp=0))\n",
        "# arena = Arena.Arena(n1p, rp, game, display=OthelloGame.display)\n",
        "# MCTS_result = arena.playGames(num_games, verbose=False)\n",
        "# print(f\"\\n\\n{MCTS_result}\")\n",
        "# print(f\"\\nNumber of games won by player1 = {MCTS_result[0]}, \"\n",
        "#       f\"number of games won by player2 = {MCTS_result[1]}, out of {num_games} games\")\n",
        "# win_rate_player1 = MCTS_result[0]/num_games\n",
        "# print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "pUOuooKVgWgn"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_80778c6b.py)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "vgMxyJ3XgWgn"
      },
      "source": [
        "```\n",
        "Number of games won by player1 = 19, num of games won by player2 = 1, out of 20 games\n",
        "\n",
        "Win rate for player1 over 20 games: 95.0%\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "pusWQ6B4gWgo"
      },
      "source": [
        "### MCTS player against Value-based player"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "vIqvzZzLgWgo"
      },
      "source": [
        "print('\\n******MCTS player versus value-based player******')\n",
        "set_seed(seed=SEED)\n",
        "vp = ValueBasedPlayer(game, vnet).play  # value-based player\n",
        "arena = Arena.Arena(n1p, vp, game, display=OthelloGame.display)\n",
        "MC_result = arena.playGames(num_games, verbose=False)\n",
        "print(f\"\\n\\n{MC_result}\")\n",
        "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
        "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
        "win_rate_player1 = MC_result[0]/num_games\n",
        "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "bX-txD1WgWgo"
      },
      "source": [
        "```\n",
        "Number of games won by player1 = 14, number of games won by player2 = 6, out of 20 games\n",
        "\n",
        "Win rate for player1 over 20 games: 70.0%\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "luVhZP7BgWgo"
      },
      "source": [
        "### MCTS player against Policy-based player"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "9YpbaI_YgWgo"
      },
      "source": [
        "print('\\n******MCTS player versus policy-based player******')\n",
        "set_seed(seed=SEED)\n",
        "pp = PolicyBasedPlayer(game, pnet).play  # policy-based player\n",
        "arena = Arena.Arena(n1p, pp, game, display=OthelloGame.display)\n",
        "MC_result = arena.playGames(num_games, verbose=False)\n",
        "print(f\"\\n\\n{MC_result}\")\n",
        "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
        "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
        "win_rate_player1 = MC_result[0]/num_games\n",
        "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TtY7PMNggWgo"
      },
      "source": [
        "```\n",
        "Number of games won by player1 = 20, number of games won by player2 = 0, out of 20 games\n",
        "\n",
        "Win rate for player1 over 20 games: 100.0%\n",
        "```"
      ]
    }
  ]
}